---
title: "TP-Markdown"
output:
  html_document:
    df_print: paged
date: "2024-08-22"
editor_options:
  markdown:
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

Para el trabajo práctico se eligió una base de datos sobre la relación entre la diabetes y las distintas variables.
Esta base de datos fue obtenida a través de la página de Kaggle.
Cuenta con 16 variables, de las cuales 5 son de tipo métrico y el resto son nominales y alrededor de 100.000 observaciones, por lo cual debimos reducirlo de forma aleatoria y proporcional.
Es decir que la mitad de los individuos de las observaciones sufren de diabetes y la otra mitad, no.
Luego de realizar esto, quedaron en total 17.000 observaciones (de las cuales 8.500 son diabéticos y el resto no).
Se decidió usar este conjunto de datos para árboles de decisión ya que contiene tanto variables métricas cómo nominales, haciendo que los árboles de decisión sean perfectos para el manejo de los datos.
No sólo eso, sino que además son fáciles de leer y accesibles.
El objetivo es poder predecir a partir de otros datos si el individuo en cuestión sufre de diabetes o no y ver cómo las variables interactúan entre sí.

### Librerías necesarias

```{r}
# Instalar paquetes necesarios si no están descargados todavía
#install.packages(c("rpart", "caret", "pROC", "e1071", "ggplot2","corrplot","rpart.plot", "reshape2","reshape","htmltools","widgetframe"))

# Cargar los paquetes
library(rpart)
library(caret)
library(pROC)
library(e1071)
library(ggplot2)
library(corrplot)
library(rpart.plot)
library(reshape2) 
library(reshape) 
library(htmltools)
library(widgetframe)


```

# Dataset Diabetes

## Se cargan los datos

```{r data}
data_original = read.csv('diabetes_dataset.csv')
```

### Reducción de la cantidad de observaciones

```{r}

#Separamos los datos entre los pacientes con diabete y sin diabetes
data_sin = data_original[data_original$diabetes == 0,]
data_con = data_original[data_original$diabetes == 1,]

#contamos la cantidad de pacientes con diabetes 
cant_diabetes = nrow(data_original[data_original$diabetes > 0, ])

# Sabemos que la cantidad de gente con diabetes es menor a la cantidad sin diabetes en el dataset, balanceamos esto y hacemos que seaa 50/50 la cantidad de casos con diabetes y sin diabetes (ponemos el seed para reproducibilidad )
set.seed(40)
if(nrow(data_sin) > cant_diabetes) {
  data_sin <- data_sin[sample(nrow(data_sin), cant_diabetes, replace = FALSE), ]
}

#juntamos la data y al guardamos
data <- rbind(data_con, data_sin)



```

# 2. Preparación de los datos

### Pre-procesamos las columnas categóricas

```{r}
# Borramos "loation"
data_sin_location <- subset(data, select = -location)

# Convertir columnas a factores y manejar valores desconocidos

data$gender <- factor(data$gender, levels = c("Male", "Female", "Unknown"))
data$gender[data$gender == "Other"] <- "Unknown"

data$location <- as.factor(data$location)

#data$smoking_history <- as.factor(data$smoking_history)

data$diabetes <- as.factor(data$diabetes)

# Se crea una nueva y única columna para la raza


data$race <- apply(data[, c("race.AfricanAmerican", "race.Asian", "race.Caucasian", "race.Hispanic", "race.Other")], 1, function(row) {
  race <- names(which(row == 1))
  if (length(race) == 0) return(NA)
  return(race)
})


```

## Summary

```{r}
summary(data)
```

## Visualizaciones

```{r}
hist(data$age,
     main = "Distribución de Edad",
     xlab = "Edad",
     ylab = "Frecuencia",
     col = "skyblue",       
     border = "white",      
     breaks = 20,          
     las = 1,              
     freq = TRUE)


gender_counts <- table(data$gender)

# Crear un gráfico de barras de la tabla de frecuencias
barplot(gender_counts,
        main = "Distribución por Género",
        xlab = "Género",
        ylab = "Frecuencia",
        col = c("lightblue", "lightgreen"), 
        border = "black",
        las = 1,                             
        cex.names = 0.8,                      
        ylim = c(0, max(gender_counts) * 1.2))


data$race <- apply(data[, c("race.AfricanAmerican", "race.Asian", "race.Caucasian", "race.Hispanic", "race.Other")], 1, function(row) {
  race <- names(which(row == 1))
  if (length(race) == 0) return(NA)  # Manejo de valores NA si ninguna raza está seleccionada
  return(race)
})

ggplot(data, aes(x = race, fill = race)) +
  geom_bar() +
  labs(title = "Distribución de Raza", x = "Raza", y = "Frecuencia") +
  theme_minimal()

# Histograma del IMC

ggplot(data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
  labs(title = "Distribución del IMC", x = "IMC", y = "Frecuencia") +
  theme_minimal()

# Boxplot de Edad por Género

ggplot(data, aes(x = gender, y = age, fill = gender)) +
  geom_boxplot() +
  labs(title = "Distribución de Edad por Género", x = "Género", y = "Edad") +
  theme_minimal()

# Gráfico de barras para la variable 'smoking' (historial de tabaquismo)

smoking_counts <- table(data$smoking_history)
  
barplot(smoking_counts,
        main = "Distribución del Historial de Tabaquismo",
        xlab = "Historial de Tabaquismo",
        ylab = "Frecuencia",
        col = c("lightcoral", "lightblue", "lightgreen", "lightyellow", "lightgray"),
        border = "black",
        las = 1,
        cex.names = 0.8,
        ylim = c(0, max(smoking_counts) * 1.2))

# Gráfico de barras para la variable 'location'

location_counts <- table(data$location)
  
barplot(location_counts,
          main = "Distribución por Ubicación",
          xlab = "Ubicación",
          ylab = "Frecuencia",
          col = rainbow(length(location_counts)),
          border = "black",
          las = 1,
          cex.names = 0.8,
          ylim = c(0, max(location_counts) * 1.2))


# Comparación de 'smoking_history' con 'diabetes'
ggplot(data, aes(x = smoking_history, fill = diabetes)) +
    geom_bar(position = "fill") +
    labs(title = "Comparación de Historial de Tabaquismo y Diabetes",
         x = "Historial de Tabaquismo",
         y = "Proporción",
         fill = "Diabetes") +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Comparación de 'location' con 'diabetes'
ggplot(data, aes(x = location, fill = diabetes)) +
    geom_bar(position = "fill") +
    labs(title = "Comparación de Ubicación y Diabetes",
         x = "Ubicación",
         y = "Proporción",
         fill = "Diabetes") +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))  


# 1. Filtrar solo las columnas numéricas

data$year <- NULL

df_numeric <- data[, sapply(data, is.numeric)]

# 2. Crear la matriz de correlación

cor_matrix <- cor(df_numeric, use = "complete.obs")

# 3. Visualizar la matriz de correlación con corrplot

corrplot(cor_matrix, method = "color", tl.col = "black", tl.cex = 0.8)


```

![ ](Proporcion%20segun%20estado.png)

# Observaciones

Son registros desde 2015 hasta 2022.
Tenemos distintas variables desde género, edad, dónde reside, su raza, si sufre de hipertensión, problemas del cardíacos, si es fumador, su IMC, HbA1c (hemoglobina glicosilada), glucosa en sangre y por último si sufre de diabetes o no.
De todas las observaciones, hay 9492 mujeres y 7507 hombres.
Se pudo observar que las razas y ubicaciones de los individupos a través del conjunto fueron divididas equitativamente.
El individuo promedio tiene alrededor de cincuenta años de edad.

En la matriz de correlación se puede observar que los tonos azules más oscuros tienen una correlación positiva (es decir, cerca de 1), mientras que los más rojos una correlación negativa (cerca de -1 ) y por último, los tonos más claros y casi blancos indican una correlación muy débil o casi nula.
Algo notable de esta visualización es que se puede observar que para las variables 'bmi' y 'blood_glucose_level' tienen una correlación positiva, lo cual tiene mucho sentido dado el contexto de salud ya que los mayores índices de masa corporal se asocian a mayores niveles de glucosal.

# 3: Construcción árbol básico

Los datos fueron divididos en tres partes (70%, 15% y 15%) de forma aleatoria con una seed para destinar cada una de las partes a diferentes etapas del proceso.

```{r}
split_dataset <- function(dataset, train_size = 0.7, validation_size = 0.15, test_size = 0.15, seed = 123) {
  # Verificar que los tamaños sumen 1
  if (train_size + validation_size + test_size != 1) {
    stop("Los tamaños de train, validation y test deben sumar 1")
  }
  
  # Configurar la semilla para reproducibilidad, si se proporciona
  if (!is.null(seed)) {
    set.seed(seed)
  }
  
  # Número total de filas en el dataset
  num_rows <- nrow(dataset)
  
  # Barajar las filas del dataset
  shuffled_indices <- sample(1:num_rows)
  
  # Calcular los tamaños de los conjuntos
  train_index <- 1:round(train_size * num_rows)
  validation_index <- (length(train_index) + 1):(length(train_index) + round(validation_size * num_rows))
  test_index <- (length(validation_index) + length(train_index) + 1):num_rows
  
  # Dividir el dataset en tres subconjuntos
  train_set <- dataset[shuffled_indices[train_index], ]
  validation_set <- dataset[shuffled_indices[validation_index], ]
  test_set <- dataset[shuffled_indices[test_index], ]

  # Devolver los subconjuntos en una lista
  return(list(train = train_set, validation = validation_set, test = test_set))
}

set = split_dataset(data)


trainData = set$train
validData = set$validation
testData = set$test

```

## Se crea un árbol de decision a través de "rpart"

```{r}

default_tree <- rpart(diabetes ~ ., data = trainData , method = "class")

```

Los hiperparámetros por default de rpart son:

```{r}
rpart.control()

# Resumen del modelo
print(default_tree)
printcp(default_tree)
```

## Visualización del árbol

```{r}
plot(default_tree)
text(default_tree, use.n = TRUE, cex = 0.8)
```

## Explicación de la estructura del árbol

Este árbol comienza por el nodo raíz o padre.
El primer corte se realiza en la variable 'blood_glucose_level' con un umbral de 210.
Es decir que la primera clasificación de los datos ocurre así: Si 'blood_glucose_level' \< 210, el árbol sigue por el lado izquierdo.
Si 'blood_glucose_level' \>= 210, el árbol sigue por el lado derecho.
Luego, el segundo nivel de decisión, del lado izquierdo, se basa en la variable 'hbA1c_level' con un umbral de 5.35 Si 'hbA1c_level \< 5.35', indica que no tiene diabetes Si 'hbA1c_level \>= 5.35' sigue con la variable 'age' y un umbral de 43.5 Del lado derecho, el nodo que queda es un nodo terminal.
Esto sugiere que hay una clasificación directa.
Por último, al final de todo se encuentran todos los nodos terminales que ya no poseen ninguna otra división sino una clasificación.

# 4. Evaluación del árbol básico

## Feature Importance

```{r}

tree_best_features = function(tree){
  # Obtener la importancia de las variables
  importance <- tree$variable.importance
  # Ver las importancias
  print(importance)
}

tree_best_features(default_tree)

```

### Interpretacion

Con base en la importancia de las variables del árbol de decisión, se pueden extraer las siguientes conclusiones sobre las características más influyentes en el modelo:

hbA1c_level es la variable más influyente, con una importancia de 2214.68.
Esto sugiere que los niveles de hemoglobina A1c (un marcador clave para el control de la diabetes) son el factor más determinante en las predicciones del modelo.

blood_glucose_level también tiene una alta importancia (1528.64), lo que indica que los niveles de glucosa en sangre son otro factor crítico en el modelo, aunque un poco menos influyente que hbA1c_level.

age (467.57) y bmi (154.78) también contribuyen al modelo, pero en menor medida.
La edad y el índice de masa corporal (IMC) siguen siendo relevantes, pero su impacto es considerablemente menor que el de los marcadores específicos de la diabetes.

hypertension (45.62) y smoking_history (40.28) tienen una influencia limitada en el modelo, lo que sugiere que, aunque la hipertensión y el historial de tabaquismo son factores de riesgo, su impacto en la predicción del modelo es menor en comparación con los niveles de glucosa.

## Predicciones

```{r}
# Clase predicha
predicciones <- predict(default_tree, testData, type = "class")
predicciones <- factor(predicciones)
testData$diabetes <- factor(testData$diabetes)


# Ver las predicciones
#print(predicciones)

```

## Confusion Matrix

La matriz de confusión permite ver el desempeño del algoritmo propuesto.

```{r}

# Se crea una matriz de confusión
conf_matrix <- confusionMatrix(predicciones, testData$diabetes)

# Accuracy
accuracy <- conf_matrix$overall["Accuracy"]
print(conf_matrix)

```

### Interpretación de los resultados:

Se puede ver que el modelo predijo correctamente 4140 instancias no tienen la condición 0 (es decir, no son diabéticos) mientras que el número de falsos negativos fue de 373.
Para las instancias de verdaderos positivos, acertó 4281 y clasificó erroneamente 556 (falsos positivos).
A partir de estos números anteriores se pudo calcular la Sensitivity y Specificity.
Que son las proporciones en las que el modelo predijo bien para verdaderos positivos y verdaderos negativos.
Estas fueron de 88.16% y 91.99% respectivamente.
Luego, la Accuracy fue de 0.9006 o 90.06%, pero, ¿qué significa esto?
Esto implica que de todas las instancias totales el modelo predijo correctamente 90.06% de ella.
Además como las clases están balanceadas, esto refleja que el modelo está haciendo efectivamente su trabajo.

## Precision & Recall

```{r}
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]

print(precision)
print(recall)

```

Interpretación: Una precision de 95.66% fueron aquellas instancias en las que el modelo predijo que eran positivas y realmente lo eran.
Mientras que el recall fue de 97.24%.
Un precision y recall alto implica que el modelo predice correctamente la mayoría de los positivos sin perder muchos casos positivos reales.

## f1- Score

```{r}
# F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
print(f1_score)

```

Interpretación: Esto refleja que hay un buen balance entre precision y recall.
Por consecuencia, el model tiene un buen manejo sobre tanto para falsos positivos como para falsos negativos.

## AUC-ROC: Área bajo la curva ROC.

```{r}


# Probabilidad predicha
predicciones_prob <- predict(default_tree, testData, type = "prob")
probas_positivas = predicciones_prob[,2]
# Calcular el ROC y AUC
roc_curve <- roc(testData$diabetes, probas_positivas, direction = "<")
auc_roc <- auc(roc_curve)

# Mostrar el AUC-ROC
print(auc_roc)

# Graficar la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

# Agregar la línea diagonal de no discriminación (es decir, el azar)
abline(a = 0, b = 1, lty =2,col="red")
```

Interpretación de la curva: El AUC de 0.9674 muestra que el modelo tiene un buen rendimiento para la clasificación binaria, con alta sensibilidad y especificidad en la mayoría de los umbrales.
La curva ROC indica que el modelo discrimina bien entre casos positivos y negativos, superando claramente la línea de no discriminación (AUC = 0.5).

### 

# 5. Optimizacion

## Búsqueda de Hiperparámetros

Definimos los candidatos para cada hiperparámetros de los árboles de decision

# Hiperparámetros para árbol de decisión

### 1. 'minSplit'

**Descripción:** Función para medir la cálidad de una división.\
**Candidatos:** - `10` - `20` - `30` - `50`

### 3. 'minBucket'

**Descripción:** Profundidad máxima del árbol.\
**Candidatos:** - `5` - `10` - `15` - `20`

### 4. 'maxDepth'

**Descripción:** Número mínimo de muestras requeridas para dividir un nodo.\
**Candidatos:** - `2` - `5` - `10`

### 5. 'maxCompete'

**Descripción:** Número mínimo de muestras requeridas en un nodo hoja.\
**Candidatos:** - `1` - `3` - `5` - `10`

```{r}
optimizacion <- function(dataset, grid){
  # Crear un dataframe para almacenar los resultados de cada combinación
  results <- data.frame(
    minsplit = numeric(),
    minbucket = numeric(),
    maxdepth = numeric(),
    maxcompete = numeric(),
    auc = numeric()
  )
  # Se inicializan variables para almacenar el mejor modelo y su rendimiento
  
  best_auc <- 0
  best_model <- NULL
  best_params <- list()
  
  # For para probar todas las combinaciones de hiperparámetros
  for (i in 1:nrow(grid)) {
    
    # Extraer los hiperparámetros actuales
    params <- grid[i, ]
    
    # Configurar los parámetros de control de rpart
    #cp y xval son 0 para poder construir árboles con la máxima profundidad
    control <- rpart.control(
      cp = 0,                       
      minsplit = params$minsplit,
      minbucket = params$minbucket,
      maxdepth = params$maxdepth,
      xval = 0                      
    )
    
    # Se entrena al modelo usando rpart con los hiperparámetros actuales
    model <- rpart(diabetes ~ ., data = trainData, method = "class", control = control)
    
    # Se predecir en el conjunto de validación y calcular AUC-ROC
    train_pred <- predict(model, newdata = trainData, type = "prob")[,2]
    auc_valid <- auc(roc(trainData$diabetes, train_pred))
    
     # Guardar los resultados en el dataframe
    results <- rbind(results, data.frame(
      minsplit = params$minsplit,
      minbucket = params$minbucket,
      maxdepth = params$maxdepth,
      maxcompete = params$maxcompete,
      auc = auc_valid
    ))
    
    # Actualizar el mejor modelo si el actual es mejor
    if (auc_valid > best_auc) {
      best_auc <- auc_valid
      best_model <- model
      best_params <- params
    }
    
    # Se imprime el progreso
    #cat(i, "/", nrow(grid), "- AUC-ROC:", auc_valid, "\n")
  }
  
  return (list(AUC = best_auc, model = best_model, hyperParameters= best_params, allResults = results))
}

# Se crea una combinación de todos los hiperparámetros
grid <- expand.grid(
  minsplit = c(10, 20, 30, 50),
  minbucket = c(5, 10, 15, 20),
  maxdepth = c(2, 5, 10, 15, 20, 25),
  maxcompete = c(1, 3, 5, 10)
)

modelo_optimizado = optimizacion(data, grid)

best_auc = modelo_optimizado$AUC
best_model = modelo_optimizado$model
best_params = modelo_optimizado$hyperParameters
results = modelo_optimizado$allResults
  
# La mejor AUC-ROC e hiperparámetros optimizados del modelo son:
cat("Mejor AUC-ROC:", best_auc, "\n")
cat("Mejores Hiperparámeteros:", "\n")
print(best_params)
```

## Visualizamos los resultados de la optimizacion

```{r}

# Gráfico de calor para visualizar la relación entre maxdepth y minsplit con AUC-ROC
ggplot(results, aes(x = factor(maxdepth), y = factor(minsplit), fill = auc)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Relación entre Maxdepth, Minsplit y AUC-ROC",
       x = "Maxdepth",
       y = "Minsplit") +
  theme_minimal()

# Gráfico de calor para visualizar la relación entre maxcompete y maxdepth con AUC-ROC
ggplot(results, aes(x = factor(maxcompete), y = factor(maxdepth), fill = auc)) +
  geom_tile() +
  scale_fill_gradient(low = "yellow", high = "purple") +
  labs(title = "Relación entre Maxcompete, Maxdepth y AUC-ROC",
       x = "Maxcompete",
       y = "Maxdepth") +
  theme_minimal()

# Gráfico de líneas para observar cómo varía el AUC-ROC con diferentes combinaciones de hiperparámetros
ggplot(results, aes(x = maxdepth, y = auc, color = factor(minsplit))) +
  geom_line() +
  geom_point() +
  labs(title = "AUC-ROC vs Maxdepth para diferentes Minsplit",
       x = "Maxdepth",
       y = "AUC-ROC",
       color = "Minsplit") +
  theme_minimal()
```

### Interpretacion

En el primer gráfico, que muestra la relación entre Maxdepth, Minsplit y AUC-ROC, observamos que a medida que Maxdepth aumenta, el AUC-ROC también se incrementa, estabilizándose en un punto alto cuando Maxdepth supera los 10.
Este comportamiento sugiere que una mayor profundidad permite al modelo capturar mejor las complejidades de los datos, mejorando su capacidad predictiva.
Sin embargo, una vez que el modelo alcanza una profundidad de alrededor de 10, el AUC-ROC se estabiliza, lo que indica que el modelo podría estar comenzando a sobreajustarse al ruido en los datos sin obtener mejoras adicionales en la capacidad predictiva.
Por otro lado, Minsplit parece tener un impacto menor; aunque un Minsplit más bajo mejora ligeramente el AUC-ROC, su efecto es menos pronunciado en comparación con el impacto de Maxdepth.

En el segundo gráfico, que explora la relación entre Maxcompete, Maxdepth y AUC-ROC, se observa que el rendimiento del modelo es más bajo cuando Maxcompete tiene valores bajos, pero mejora conforme Maxcompete aumenta.
Este patrón indica que permitir más divisiones competidoras en cada nodo (Maxcompete más alto) mejora la capacidad del árbol para encontrar divisiones óptimas, incrementando así el AUC-ROC.
La combinación de un Maxcompete más alto con un Maxdepth mayor parece proporcionar el mejor rendimiento general.
Sin embargo, hay que tener cuidado con un Maxcompete excesivamente alto, ya que podría aumentar la complejidad del modelo y llevar a un sobreajuste.

En el tercer gráfico, que ilustra la relación entre AUC-ROC y Maxdepth para diferentes valores de Minsplit, vemos que el AUC-ROC aumenta significativamente con el incremento de Maxdepth hasta un rango de 5 a 10, después de lo cual se estabiliza.
Este patrón de mejora inicial seguido de estabilización refuerza la idea de que un aumento en la profundidad permite al modelo capturar relaciones más complejas en los datos, pero más allá de un cierto punto, se alcanzan rendimientos decrecientes.
Las líneas que representan diferentes valores de Minsplit muestran una tendencia similar, lo que indica que, aunque Minsplit controla la granularidad de las divisiones, su impacto se ve opacado por la capacidad del modelo de ajustarse a la profundidad.

En general, todos los gráficos indican que existe un punto de saturación en el rendimiento del modelo, especialmente en términos de la profundidad del árbol (Maxdepth).
Esto sugiere que una estrategia óptima sería maximizar la profundidad del árbol hasta un punto razonable y ajustar los parámetros de Maxcompete y Minsplit para encontrar un equilibrio que evite tanto el subajuste como el sobreajuste.
Ajustar estos parámetros puede ayudar a desarrollar un modelo más robusto y efectivo, capturando relaciones complejas mientras evita el ruido innecesario, optimizando así la predicción de diabetes en los datos.

## Testeamos el rendimiento del mejor modelo con los datos de Validacion

```{r}
rendimiento_modelo <- function(data, modelo, nombre_grafico){
  
  prediccion <- predict(modelo, newdata = data, type = "prob")[,2]
  roc_test <- roc(data$diabetes, prediccion)
  auc_test <- auc(roc_test)
  cat("AUC-ROC on test set:", auc_test, "\n")
  
  ## Visualización de la curva AUC-ROC

  # Se grafica la curva ROC
  plot(roc_test, col = "blue", lwd = 2, main = paste("Curva AUC-ROC ",nombre_grafico), auc.polygon = TRUE, grid = TRUE) 
  
  # Se añade la línea diagonal 
  abline(a = 0, b = 1, col = "gray", lty = 2)
  
  # Convertir 'predicciones' a factor con los mismos niveles que 'testData$diabetes'
  prediccion <- as.factor(prediccion)
  
  ##macheamos a factores 
  prediccion <- factor(prediccion, levels = levels(data$diabetes))
  
  # Crear una tabla de confusión
  conf_matrix <- confusionMatrix(prediccion, data$diabetes)
  
  # Accuracy
  accuracy <- conf_matrix$overall["Accuracy"]
  
  return (list(matConfusion = conf_matrix, accuracy = accuracy))
  
}

```

## Medicion del Auc-Roc con los datos de validacion y testeo

```{r}
#Validacion
rendimiento_validacion = rendimiento_modelo(validData,best_model,"(Validacion)")
matriz_confusion_validacion = rendimiento_validacion$matConfusion
print(matriz_confusion_validacion)
```

```{r}
#Testeo
rendimiento_testeo = rendimiento_modelo(testData,best_model, "(Testeo)")
matriz_confusion_testeo = rendimiento_testeo$matConfusion
print(matriz_confusion_testeo)

```

## Interpretación:

En comparación a los resultados de la matriz de confusión anterior, se puede notar que ha habido una mejora notable entre ambos modelos.
La accuracy ha aumentado a un 96.2%, esto significa que el modelo predice bien en todas las instancias.
También la sensitivity paso a ser 95.85%, esto implica que el modelo identifica todas las instancias positivas de forma correcta sin ningún falso negativo.
La specificity ahora también es 96.51%, lo que demuestra que el modelo reconoce todas las instancias negativas sin ningún falso positivo.

# 6. Interpretación de los resultados

A la hora de comparar ambas curvas, se puede observar que en esta segunda curva (que contiene lo hiperparámetros optimizados), que se acerca mucho más al vértice de la izquierda.
Esto podría indicar robustez, es decir, que no es tan sensible a cambios en los datos de entrada.
Otra diferencia es que podemos ver que el área debajo de la curva en la segunda curva es muy cercano a 1, lo cual señala un muy buen rendimiento mientras que en la primera se nota que es un poco más pequeña.
Además, otra diferencia importante es que en la curva del modelo optimizado se puede abstraer que este puede realizar mejores distinciones entre clases positivas y negativas, con muy pocos falsos negativos y falsos positivos.

## Visualizamos el Arbol

```{r}
#cex controla el tamaño del texto en el gráfico.
#tweak ajusta el tamaño de la gráfica en general.


rpart.plot(model, type = 3, extra = 104, fallen.leaves = TRUE, cex = 0.8, tweak = 1.2)


```

# 7. Análisis del impacto de valores faltantes (20% 50% 70%)

```{r}
# Función que introduce NA 
dataset_NAgenerator <- function(dataset, rate) {
  # Verificar que el rate esté entre 1 y 100
  if (rate < 1 | rate > 100) {
    stop("El 'rate' debe estar entre 1 y 100")
  }
  
  # Se hace una copia del dataset para no modificar el original
  dataset_na <- dataset
  
  # Se convierte el rate a un porcentaje
  rate <- rate / 100
  
  # El número de filas en el dataset
  num_rows <- nrow(dataset_na)
  
  # El número de filas a reemplazar por NA para la columna actual
    num_na <- round(num_rows * rate)
  
  # Se iterar sobre cada columna
  for (col in colnames(dataset_na)) {
    # Se saltea la columna 'diabetes'
    if (col != 'diabetes') {
      # Se seleccionan filas aleatorias para reemplazar por NA en la columna actual
      rows_to_na <- sample(1:num_rows, num_na)
      
      # Se reemplaza las filas seleccionadas por NA en la columna actual
      dataset_na[rows_to_na, col] <- NA
    }
    
    
  }
  
  return(dataset_na)
}
```

## Generamos los 3 conjuntos de datos

```{r}
set_20 = split_dataset(data)
train_20 = set_20$train
train_20 = dataset_NAgenerator(train_20,20)
validation_20 = set_20$validation
validation_20 = dataset_NAgenerator(validation_20,20)
test_20 = set_20$test
test_20 = dataset_NAgenerator(test_20,20)

set_50 = split_dataset(data)
train_50 = set_50$train
train_50 = dataset_NAgenerator(train_50,50)
validation_50 = set_50$validation
validation_50 = dataset_NAgenerator(validation_50,50)
test_50 = set_50$test
test_50 = dataset_NAgenerator(test_50,50)

set_70 = split_dataset(data)
train_70 = set_70$train
train_70 = dataset_NAgenerator(train_70,70)
validation_70 = set_70$validation
validation_70 = dataset_NAgenerator(validation_70,70)
test_70 = set_70$test
test_70 = dataset_NAgenerator(test_70,70)


```

## Se entrenan y prueban con los 3 datasets

```{r}
#Esto es todo lo que deberiamos hacer oara cad auno de los data sets
test_dataset = function(train,valid,test,grid,porcentaje){
  modelo_optimizado = optimizacion(train, grid)

  best_auc = modelo_optimizado$AUC
  best_model = modelo_optimizado$model
  best_params = modelo_optimizado$hyperParameters
  results = modelo_optimizado$allResults
  
  #Validacion
  rendimiento_validacion = rendimiento_modelo(valid,best_model,paste("- Validación (NAs ", porcentaje,"%)"))
  matriz_confusion_validacion = rendimiento_validacion$matConfusion
  #print(matriz_confusion_validacion) paste("- Validación (NAs ", porcentaje,"%)")
  
  #Testeo
  rendimiento_testeo = rendimiento_modelo(test,best_model, paste("- Test (NAs ", porcentaje,"%)"))
  matriz_confusion_testeo = rendimiento_testeo$matConfusion
  #print(matriz_confusion_testeo)
  
  return (list(matConfValid = matriz_confusion_validacion,matConfTest = matriz_confusion_testeo))
}


matrices_data20 = test_dataset(train_20,validation_20,test_20,grid,"20")

print(matrices_data20$matConfValid)
print(matrices_data20$matConfTest)

matrices_data50 = test_dataset(train_50,validation_20,test_50,grid,"50")
matrices_data70 = test_dataset(train_70,validation_20,test_70,grid,"70")

```

## Comparacion de los resultados en testeo

```{r}
# Creación del dataframe con las métricas de cada matriz de confusión
df_metricas <- data.frame(
  Porcentaje_Faltantes = c("0% (original)", "20%", "50%", "70%"),
  Accuracy = c(0.9634, 0.942, 0.9179, 0.8655),
  Sensitivity = c(0.9557, 0.9359, 0.8953, 0.7954),
  Specificity = c(0.9702, 0.9473, 0.9365, 0.9286)
)

print(df_metricas)
```

```{r}

# Convertir el dataframe a formato largo para ggplot2
df_metricas_largo <- melt(df_metricas, id.vars = "Porcentaje_Faltantes")

# Gráfico de barras
ggplot(df_metricas_largo, aes(x = Porcentaje_Faltantes, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Evaluación de Metricas según el Porcentaje de Datos Faltantes",
       x = "Porcentaje de Datos Faltantes",
       y = "Valor de la Métrica",
       fill = "Métrica") +
  theme_minimal()
```

```{r}
# Gráfico de líneas
ggplot(df_metricas_largo, aes(x = Porcentaje_Faltantes, y = value, color = variable, group = variable)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(title = "Tendencia de las Métricas según el Porcentaje de Datos Faltantes",
       x = "Porcentaje de Datos Faltantes",
       y = "Valor de la Métrica",
       color = "Métrica") +
  theme_minimal()
```

## Interpretacion:

En el primer gráfico de barras, se comparan las métricas de evaluación (Accuracy, Sensitivity y Specificity) a diferentes niveles de datos faltantes: 0% (original), 20%, 50% y 70%.
Cuando observamos los resultados con el 0% de datos faltantes, es decir, en el dataset original, todas las métricas están muy cerca del valor 1.
Esto señala que el modelo funciona con una alta precisión y capacidad de identificar correctamente tanto los verdaderos positivos como los verdaderos negativos.
Esto es útil para comparar con el resto de los datasets.

A medida que se introduce un 20% de datos faltantes, las métricas comienzan a mostrar una ligera disminución.
En particular, Sensitivity es la métrica más afectada.
La disminución en Sensitivity indica que el modelo está perdiendo algo de su capacidad para identificar correctamente los verdaderos positivos.
A medida que se aumentan los datos faltantes a un 50%, esta tendencia se acentúa: todas las métricas disminuyen significativamente.
Esto se debe a que el modelo tiene menos información para entrenarse, lo que compromete su capacidad de generalización.
El impacto es aún más evidente al observar el nivel de datos faltantes del 70%, donde Specificity se mantiene relativamente alta en comparación con Sensitivity Esto puede deberse a que el modelo, enfrentando una mayor cantidad de datos faltantes, aún puede identificar correctamente los verdaderos negativos (alta Specificity), pero le cuesta identificar los verdaderos positivos (baja Sensitivity).
Este comportamiento se debe a los datos faltantes ya que introducen un sesgo que afecta desproporcionadamente la capacidad del modelo para detectar ciertos tipos de eventos, especialmente si la clase de interés es la menos representada.

El segundo gráfico, que muestra las líneas de tendencia para cada métrica a medida que aumenta el porcentaje de datos faltantes, ofrece una visualización clara de la degradación progresiva del rendimiento del modelo.
Todas las métricas exhiben una tendencia descendente a medida que se incrementa el porcentaje de datos faltantes, dado que la falta de datos introduce incertidumbre y posibles sesgos en el entrenamiento del modelo.
Esta gráfica enfatiza que la pérdida de datos no solo afecta la Accuracy general, sino también la capacidad del modelo para diferenciar entre las clases de manera precisa y consistente.

Una observación interesante en este gráfico es que mientras que Specificity se mantiene relativamente alta en comparación con Sensitivity, esto sugiere que el modelo conserva una mayor precisión en la identificación de verdaderos negativos que de verdaderos positivos.
Este comportamiento puede deberse a que el modelo se vuelve más conservador en sus predicciones cuando enfrenta datos incompletos, prefiriendo no predecir un positivo en presencia de incertidumbre.

En conclusión, ambos gráficos reflejan claramente que un aumento en los datos faltantes afecta negativamente el rendimiento de tu modelo de clasificación, con efectos más pronunciados en la Sensitivity. Esto resalta la importancia de manejar adecuadamente los datos faltantes para minimizar su impacto negativo en la capacidad de predicción y generalización del modelo.
Para mitigar estos efectos, se podria implementar técnicas avanzadas de imputación de datos o utilizar modelos que sean más robustos ante la falta de datos.

# 8. Conclusiones y discusión

El análisis sugiere que los niveles de glucosa en sangre deberían reducirse como medida general de salud para la prevención de diabetes.
Además, se observó que la raza de los participantes, a pesar de ser recolectada en el estudio, tuvo poca influencia en la predicción de diabetes. Este resultado podría reflejar un sesgo en la recolección de datos en Estados Unidos, donde a menudo se consideran variables como la raza, incluso cuando su impacto en los resultados es mínimo.
Por lo tanto, es importante cuestionar la relevancia de ciertas variables en la construcción de modelos predictivos.

El modelo de árbol de decisión fue altamente efectivo para predecir si una persona tiene diabetes, lo que se demuestra en los altos valores de precisión (accuracy) obtenidos en las matrices de confusión para diferentes subconjuntos de datos.
Esto indica que el modelo se ajusta bien tanto a los datos de entrenamiento como de prueba, mostrando un desempeño consistente en términos de precisión y fiabilidad.
A lo largo del proceso de evaluación, las métricas clave, como la precisión, se mantuvieron altas, lo que sugiere que el modelo es una herramienta robusta y estable para el análisis predictivo, incluso cuando enfrenta desafíos como la presencia de valores faltantes.

Para mejorar el modelo, sería útil explorar configuraciones alternativas de hiperparámetros que puedan optimizar aún más su rendimiento.
Una búsqueda más exhaustiva de hiperparámetros podría descubrir configuraciones que ajusten mejor el modelo a los datos.
Además, investigar la relación entre fumar y la prevalencia de diabetes podría ofrecer nuevas perspectivas, especialmente considerando que "fumar" fue una de las variables con más datos faltantes.
La recolección de datos más completos o el uso de técnicas avanzadas de imputación para llenar los vacíos podría mejorar el rendimiento del modelo.
Asimismo, es crucial gestionar adecuadamente los valores faltantes para mantener la calidad del análisis y mejorar la precisión del modelo en aplicaciones prácticas futuras.
