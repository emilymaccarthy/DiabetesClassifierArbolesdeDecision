---
title: "TP-Markdown"
output: pdf_document
date: "2024-08-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

Para el trabajo práctico se eligió una base de datos sobre la relación entre la diabetes y las distintas variables. Esta base de datos fue obtenida a través de la página de Kaggle. Cuenta con 16 variables, de las cuales 5 son de tipo métrico y el resto son nominales y alrededor de 100.000 observaciones, por lo cual debimos reducirlo de forma aleatoria y proporcional. Es decir que la mitad de los individuos de las observaciones sufren de diabetes y la otra mitad, no. Luego de realizar esto, quedaron en total 17.000 observaciones (de las cuales 8.500 son diabéticos y el resto no). Se decidió usar este conjunto de datos para árboles de decisión ya que contiene tanto variables métricas cómo nominales, haciendo que los árboles de decisión sean perfectos para el manejo de los datos. No sólo eso, sino que además son fáciles de leer y accesibles. El objetivo es poder predecir a partir de otros datos si el individuo en cuestión sufre de diabetes o no y ver cómo las variables interactúan entre sí.

### Librerías necesarias

```{r}
# Instalar paquetes necesarios si no están descargados todavía
#install.packages(c("rpart", "caret", "pROC", "e1071"))

# Cargar los paquetes
library(rpart)
library(caret)
library(pROC)
library(e1071)
library(ggplot2)
library(corrplot)
library(rpart.plot)

```

# Dataset Diabetes

## Se cargan los datos

```{r data}
data_original = read.csv('diabetes_dataset.csv')
```

### Reducción de la cantidad de observaciones

```{r}

#Separamos los datos entre los pacientes con diabete y sin diabetes
data_sin = data_original[data_original$diabetes == 0,]
data_con = data_original[data_original$diabetes == 1,]

#contamos la cantidad de pacientes con diabetes 
cant_diabetes = nrow(data_original[data_original$diabetes > 0, ])

# Sabemos que la cantidad de gente con diabetes es menor a la cantidad sin diabetes en el dataset, balanceamos esto y hacemos que seaa 50/50 la cantidad de casos con diabetes y sin diabetes (ponemos el seed para reproducibilidad )
set.seed(40)
if(nrow(data_sin) > cant_diabetes) {
  data_sin <- data_sin[sample(nrow(data_sin), cant_diabetes, replace = FALSE), ]
}

#juntamos la data y al guardamos
data <- rbind(data_con, data_sin)



```

# 2. Preparación de los datos

### Pre-procesamos las columnas categóricas

```{r}
# Borramos "loation"
data_sin_location <- subset(data, select = -location)

# Convertir columnas a factores y manejar valores desconocidos

data$gender <- factor(data$gender, levels = c("Male", "Female", "Unknown"))
data$gender[data$gender == "Other"] <- "Unknown"

data$location <- as.factor(data$location)

#data$smoking_history <- as.factor(data$smoking_history)

data$diabetes <- as.factor(data$diabetes)

# Se crea una nueva y única columna para la raza


data$race <- apply(data[, c("race.AfricanAmerican", "race.Asian", "race.Caucasian", "race.Hispanic", "race.Other")], 1, function(row) {
  race <- names(which(row == 1))
  if (length(race) == 0) return(NA)
  return(race)
})


```

## Summary

```{r}
summary(data)
```

## Visualizaciones

```{r}
hist(data$age,
     main = "Distribución de Edad",
     xlab = "Edad",
     ylab = "Frecuencia",
     col = "skyblue",       
     border = "white",      
     breaks = 20,          
     las = 1,              
     freq = TRUE)


gender_counts <- table(data$gender)

# Crear un gráfico de barras de la tabla de frecuencias
barplot(gender_counts,
        main = "Distribución por Género",
        xlab = "Género",
        ylab = "Frecuencia",
        col = c("lightblue", "lightgreen"), 
        border = "black",
        las = 1,                             
        cex.names = 0.8,                      
        ylim = c(0, max(gender_counts) * 1.2))


data$race <- apply(data[, c("race.AfricanAmerican", "race.Asian", "race.Caucasian", "race.Hispanic", "race.Other")], 1, function(row) {
  race <- names(which(row == 1))
  if (length(race) == 0) return(NA)  # Manejo de valores NA si ninguna raza está seleccionada
  return(race)
})

ggplot(data, aes(x = race, fill = race)) +
  geom_bar() +
  labs(title = "Distribución de Raza", x = "Raza", y = "Frecuencia") +
  theme_minimal()

# Histograma del IMC

ggplot(data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
  labs(title = "Distribución del IMC", x = "IMC", y = "Frecuencia") +
  theme_minimal()

# Boxplot de Edad por Género

ggplot(data, aes(x = gender, y = age, fill = gender)) +
  geom_boxplot() +
  labs(title = "Distribución de Edad por Género", x = "Género", y = "Edad") +
  theme_minimal()

# 1. Filtrar solo las columnas numéricas

data$year <- NULL

df_numeric <- data[, sapply(data, is.numeric)]

# 2. Crear la matriz de correlación

cor_matrix <- cor(df_numeric, use = "complete.obs")

# 3. Visualizar la matriz de correlación con corrplot

corrplot(cor_matrix, method = "color", tl.col = "black", tl.cex = 0.8)


```

# Observaciones

Son registros desde 2015 hasta 2022. Tenemos distintas variables desde género, edad, dónde reside, su raza, si sufre de hipertensión, problemas del cardíacos, si es fumador, su IMC, HbA1c (hemoglobina glicosilada), glucosa en sangre y por último si sufre de diabetes o no. De todas las observaciones, hay 9492 mujeres y 7507 hombres. Se pudo observar que las razas y ubicaciones de los individupos a través del conjunto fueron divididas equitativamente. El individuo promedio tiene alrededor de cincuenta años de edad.

En la matriz de correlación se puede observar que los tonos azules más oscuros tienen una correlación positiva (es decir, cerca de 1), mientras que los más rojos una correlación negativa (cerca de -1 ) y por último, los tonos más claros y casi blancos indican una correlación muy débil o casi nula. Algo notable de esta visualización es que se puede observar que para las variables 'bmi' y 'blood_glucose_level' tienen una correlación positiva, lo cual tiene mucho sentido dado el contexto de salud ya que los mayores índices de masa corporal se asocian a mayores niveles de glucosal.

# 3: Construcción árbol básico

Los datos fueron divididos en tres partes (70%, 15% y 15%) de forma aleatoria con una seed para destinar cada una de las partes a diferentes etapas del proceso.

```{r}
split_dataset <- function(dataset, train_size = 0.7, validation_size = 0.15, test_size = 0.15, seed = 123) {
  # Verificar que los tamaños sumen 1
  if (train_size + validation_size + test_size != 1) {
    stop("Los tamaños de train, validation y test deben sumar 1")
  }
  
  # Configurar la semilla para reproducibilidad, si se proporciona
  if (!is.null(seed)) {
    set.seed(seed)
  }
  
  # Número total de filas en el dataset
  num_rows <- nrow(dataset)
  
  # Barajar las filas del dataset
  shuffled_indices <- sample(1:num_rows)
  
  # Calcular los tamaños de los conjuntos
  train_index <- 1:round(train_size * num_rows)
  validation_index <- (length(train_index) + 1):(length(train_index) + round(validation_size * num_rows))
  test_index <- (length(validation_index) + length(train_index) + 1):num_rows
  
  # Dividir el dataset en tres subconjuntos
  train_set <- dataset[shuffled_indices[train_index], ]
  validation_set <- dataset[shuffled_indices[validation_index], ]
  test_set <- dataset[shuffled_indices[test_index], ]

  # Devolver los subconjuntos en una lista
  return(list(train = train_set, validation = validation_set, test = test_set))
}

set = split_dataset(data)


trainData = set$train
validData = set$validation
testData = set$test

```



## Se crea un árbol de decision a través de "rpart"

```{r}

default_tree <- rpart(diabetes ~ ., data = trainData , method = "class")

```


Los hiperparámetros por default de rpart son:

```{r}
rpart.control()

# Resumen del modelo
print(default_tree)
printcp(default_tree)
```

## Visualización del árbol

```{r}
plot(default_tree)
text(default_tree, use.n = TRUE, cex = 0.8)
```
## Explicación de la estructura del árbol
Este árbol comienza por el nodo raíz o padre. El primer corte se realiza en la variable 'blood_glucose_level' con un umbral de 210. Es decir que la primera clasificación de los datos ocurre así:
Si 'blood_glucose_level' < 210, el árbol sigue por el lado izquierdo.
Si 'blood_glucose_level' >= 210, el árbol sigue por el lado derecho.
Luego, el segundo nivel de decisión, del lado izquierdo, se basa en la variable 'hbA1c_level' con un umbral de 5.35
Si 'hbA1c_level < 5.35', indica que no tiene diabetes
Si 'hbA1c_level >= 5.35' sigue con la variable 'age' y un umbral de 43.5
Del lado derecho, el nodo que queda es un nodo terminal. Esto sugiere que hay una clasificación directa.
Por último, al final de todo se encuentran todos los nodos terminales que ya no poseen ninguna otra división sino una clasificación.


# 4. Evaluación del árbol básico

## Feature Importance
```{r}

tree_best_features = function(tree){
  # Obtener la importancia de las variables
  importance <- tree$variable.importance
  # Ver las importancias
  print(importance)
}

tree_best_features(default_tree)

```

### Interppretacion
texto...

## Predicciones

```{r}
# Clase predicha
predicciones <- predict(default_tree, testData, type = "class")
predicciones <- factor(predicciones)
testData$diabetes <- factor(testData$diabetes)


# Ver las predicciones
#print(predicciones)

```

## Confusion Matrix

La matriz de confusión permite ver el desempeño del algoritmo propuesto.

```{r}

# Se crea una matriz de confusión
conf_matrix <- confusionMatrix(predicciones, testData$diabetes)

# Accuracy
accuracy <- conf_matrix$overall["Accuracy"]
print(conf_matrix)

```



Interpretación de los resultados:
Se puede ver que el modelo predijo correctamente 4140 instancias no tienen la condición 0 (es decir, no son diabéticos) mientras que el número de falsos negativos fue de 373. Para las instancias de verdaderos positivos, acertó 4281 y clasificó erroneamente 556 (falsos positivos).
A partir de estos números anteriores se pudo calcular la Sensitivity y Specificity. Que son las proporciones en las que el modelo predijo bien para verdaderos positivos y verdaderos negativos. Estas fueron de 88.16% y 91.99% respectivamente.
Luego, la Accuracy fue de 0.9006 o 90.06%, pero, ¿qué significa esto? Esto implica que de todas las instancias totales el modelo predijo correctamente 90.06% de ella. Además como las clases están balanceadas, esto refleja que el modelo está haciendo efectivamente su trabajo.


## Precision & Recall

```{r}
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]

print(precision)
print(recall)

```
Interpretación:
Una precision de 95.66% fueron aquellas instancias en las que el modelo predijo que eran positivas y realmente lo eran. Mientras que el recall fue de 97.24%.
Un precision y recall alto implica que el modelo predice correctamente la mayoría de los positivos sin perder muchos casos positivos reales.

## f1- Score

```{r}
# F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
print(f1_score)

```
Interpretación:
Esto refleja que hay un buen balance entre precision y recall. Por consecuencia, el model tiene un buen manejo sobre tanto para falsos positivos como para falsos negativos.

## AUC-ROC: Área bajo la curva ROC.

```{r}


# Probabilidad predicha
predicciones_prob <- predict(default_tree, testData, type = "prob")
probas_positivas = predicciones_prob[,2]
# Calcular el ROC y AUC
roc_curve <- roc(testData$diabetes, probas_positivas, direction = "<")
auc_roc <- auc(roc_curve)

# Mostrar el AUC-ROC
print(auc_roc)

# Graficar la curva ROC
plot(roc_curve, main = "Curva ROC", col = "blue", lwd = 2)

# Agregar la línea diagonal de no discriminación (es decir, el azar)
abline(a = 0, b = 1, lty =2,col="red")
```
Interpretación de la curva: El AUC de 0.9674 muestra que el modelo tiene un buen rendimiento para la clasificación binaria, con alta sensibilidad y especificidad en la mayoría de los umbrales. La curva ROC indica que el modelo discrimina bien entre casos positivos y negativos, superando claramente la línea de no discriminación (AUC = 0.5).


###
# 5. Optimizacion

## Búsqueda de Hiperparámetros

Definimos los candidatos para cada hiperparámetros de los árboles de decision

# Hiperparámetros para árbol de decisión

### 1. 'minSplit'

**Descripción:** Función para medir la cálidad de una división.\
**Candidatos:** - `10` - `20` - `30` - `50`

### 3. 'minBucket'

**Descripción:** Profundidad máxima del árbol.\
**Candidatos:** - `5` - `10` - `15` - `20`

### 4. 'maxDepth'

**Descripción:** Número mínimo de muestras requeridas para dividir un nodo.\
**Candidatos:** - `2` - `5` - `10`

### 5. 'maxCompete'

**Descripción:** Número mínimo de muestras requeridas en un nodo hoja.\
**Candidatos:** - `1` - `3` - `5` - `10`

```{r}
optimizacion <- function(dataset, grid){
  # Crear un dataframe para almacenar los resultados de cada combinación
  results <- data.frame(
    minsplit = numeric(),
    minbucket = numeric(),
    maxdepth = numeric(),
    maxcompete = numeric(),
    auc = numeric()
  )
  # Se inicializan variables para almacenar el mejor modelo y su rendimiento
  
  best_auc <- 0
  best_model <- NULL
  best_params <- list()
  
  # For para probar todas las combinaciones de hiperparámetros
  for (i in 1:nrow(grid)) {
    
    # Extraer los hiperparámetros actuales
    params <- grid[i, ]
    
    # Configurar los parámetros de control de rpart
    #cp y xval son 0 para poder construir árboles con la máxima profundidad
    control <- rpart.control(
      cp = 0,                       
      minsplit = params$minsplit,
      minbucket = params$minbucket,
      maxdepth = params$maxdepth,
      xval = 0                      
    )
    
    # Se entrena al modelo usando rpart con los hiperparámetros actuales
    model <- rpart(diabetes ~ ., data = trainData, method = "class", control = control)
    
    # Se predecir en el conjunto de validación y calcular AUC-ROC
    train_pred <- predict(model, newdata = trainData, type = "prob")[,2]
    auc_valid <- auc(roc(trainData$diabetes, train_pred))
    
     # Guardar los resultados en el dataframe
    results <- rbind(results, data.frame(
      minsplit = params$minsplit,
      minbucket = params$minbucket,
      maxdepth = params$maxdepth,
      maxcompete = params$maxcompete,
      auc = auc_valid
    ))
    
    # Actualizar el mejor modelo si el actual es mejor
    if (auc_valid > best_auc) {
      best_auc <- auc_valid
      best_model <- model
      best_params <- params
    }
    
    # Se imprime el progreso
    #cat(i, "/", nrow(grid), "- AUC-ROC:", auc_valid, "\n")
  }
  
  return (list(AUC = best_auc, model = best_model, hyperParameters= best_params, allResults = results))
}

# Se crea una combinación de todos los hiperparámetros
grid <- expand.grid(
  minsplit = c(10, 20, 30, 50),
  minbucket = c(5, 10, 15, 20),
  maxdepth = c(2, 5, 10, 15, 20, 25),
  maxcompete = c(1, 3, 5, 10)
)

modelo_optimizado = optimizacion(data, grid)

best_auc = modelo_optimizado$AUC
best_model = modelo_optimizado$model
best_params = modelo_optimizado$hyperParameters
results = modelo_optimizado$allResults
  
# La mejor AUC-ROC e hiperparámetros optimizados del modelo son:
cat("Mejor AUC-ROC:", best_auc, "\n")
cat("Mejores Hiperparámeteros:", "\n")
print(best_params)
```



## Visualizamos los resultados del GridSearch

```{r}

# Gráfico de calor para visualizar la relación entre maxdepth y minsplit con AUC-ROC
ggplot(results, aes(x = factor(maxdepth), y = factor(minsplit), fill = auc)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Relación entre Maxdepth, Minsplit y AUC-ROC",
       x = "Maxdepth",
       y = "Minsplit") +
  theme_minimal()

# Gráfico de calor para visualizar la relación entre maxcompete y maxdepth con AUC-ROC
ggplot(results, aes(x = factor(maxcompete), y = factor(maxdepth), fill = auc)) +
  geom_tile() +
  scale_fill_gradient(low = "yellow", high = "purple") +
  labs(title = "Relación entre Maxcompete, Maxdepth y AUC-ROC",
       x = "Maxcompete",
       y = "Maxdepth") +
  theme_minimal()

# Gráfico de líneas para observar cómo varía el AUC-ROC con diferentes combinaciones de hiperparámetros
ggplot(results, aes(x = maxdepth, y = auc, color = factor(minsplit))) +
  geom_line() +
  geom_point() +
  labs(title = "AUC-ROC vs Maxdepth para diferentes Minsplit",
       x = "Maxdepth",
       y = "AUC-ROC",
       color = "Minsplit") +
  theme_minimal()
```


## Testeamos el rendimiento del mejor modelo con los datos de Validacion

```{r}
rendimiento_modelo <- function(data, modelo, nombre_grafico){
  
  prediccion <- predict(modelo, newdata = data, type = "prob")[,2]
  roc_test <- roc(data$diabetes, prediccion)
  auc_test <- auc(roc_test)
  cat("AUC-ROC on test set:", auc_test, "\n")
  
  ## Visualización de la curva AUC-ROC

  # Se grafica la curva ROC
  plot(roc_test, col = "blue", lwd = 2, main = paste("Curva AUC-ROC ",nombre_grafico), auc.polygon = TRUE, grid = TRUE) 
  
  # Se añade la línea diagonal 
  abline(a = 0, b = 1, col = "gray", lty = 2)
  
  # Convertir 'predicciones' a factor con los mismos niveles que 'testData$diabetes'
  prediccion <- as.factor(prediccion)
  
  ##macheamos a factores 
  prediccion <- factor(prediccion, levels = levels(data$diabetes))
  
  # Crear una tabla de confusión
  conf_matrix <- confusionMatrix(prediccion, data$diabetes)
  
  # Accuracy
  accuracy <- conf_matrix$overall["Accuracy"]
  
  return (list(matConfusion = conf_matrix, accuracy = accuracy))
  
}

```

## Medicion del Auc-Roc con los datos de validacion y testeo

```{r}
#Validacion
rendimiento_validacion = rendimiento_modelo(validData,best_model,"(Validacion)")
matriz_confusion_validacion = rendimiento_validacion$matConfusion
print(matriz_confusion_validacion)
```


```{r}
#Testeo
rendimiento_testeo = rendimiento_modelo(testData,best_model, "(Testeo)")
matriz_confusion_testeo = rendimiento_testeo$matConfusion
print(matriz_confusion_testeo)

```



## Interpretación:
En comparación a los resultados de la matriz de confusión anterior, se puede notar que ha habido una mejora notable entre ambos modelos. La accuracy ha aumentado a un 100%, esto significa que el modelo predice bien en todas las instancias. 
También la sensitivity paso a ser 1, esto implica que el modelo identifica todas las instancias positivas de forma correcta sin ningún falso negativo. La specificity ahora también es 1, lo que demuestra que el modelo reconoce todas las instancias 
negativas sin ningún falso positivo.


# 6. Interpretación de los resultados

A la hora de comparar ambas curvas, se puede observar que en esta segunda curva (que contiene lo hiperparámetros optimizados), que se acerca mucho más al vértice de la izquierda. Esto podría indicar robustez, es decir, que no es tan sensible a cambios en los datos de entrada. 
Otra diferencia es que podemos ver que el área debajo de la curva en la segunda curva es muy cercano a 1, lo cual señala un muy buen rendimiento mientras que en la primera se nota que es un poco más pequeña.
Además, otra diferencia importante es que en la curva del modelo optimizado se puede abstraer que este puede realizar mejores distinciones entre clases positivas y negativas, con muy pocos falsos negativos y falsos positivos.

## Visualizamos el Arbol
```{r}
#cex controla el tamaño del texto en el gráfico.
#tweak ajusta el tamaño de la gráfica en general.


#rpart.plot(model, type = 3, extra = 104, fallen.leaves = TRUE, cex = 0.8, tweak = 1.2)


```

# 7. Análisis del impacto de valores faltantes (20% 50% 70%) 

```{r}
# Función que introduce NA 
dataset_NAgenerator <- function(dataset, rate) {
  # Verificar que el rate esté entre 1 y 100
  if (rate < 1 | rate > 100) {
    stop("El 'rate' debe estar entre 1 y 100")
  }
  
  # Se hace una copia del dataset para no modificar el original
  dataset_na <- dataset
  
  # Se convierte el rate a un porcentaje
  rate <- rate / 100
  
  # El número de filas en el dataset
  num_rows <- nrow(dataset_na)
  
  # El número de filas a reemplazar por NA para la columna actual
    num_na <- round(num_rows * rate)
  
  # Se iterar sobre cada columna
  for (col in colnames(dataset_na)) {
    # Se saltea la columna 'diabetes'
    if (col != 'diabetes') {
      # Se seleccionan filas aleatorias para reemplazar por NA en la columna actual
      rows_to_na <- sample(1:num_rows, num_na)
      
      # Se reemplaza las filas seleccionadas por NA en la columna actual
      dataset_na[rows_to_na, col] <- NA
    }
    
    
  }
  
  return(dataset_na)
}
```

## Generamos los 3 conjuntos de datos

```{r}
set_20 = split_dataset(data)
train_20 = set_20$train
train_20 = dataset_NAgenerator(train_20,20)
validation_20 = set_20$validation
validation_20 = dataset_NAgenerator(validation_20,20)
test_20 = set_20$test
test_20 = dataset_NAgenerator(test_20,20)

set_50 = split_dataset(data)
train_50 = set_50$train
train_50 = dataset_NAgenerator(train_50,50)
validation_50 = set_50$validation
validation_50 = dataset_NAgenerator(validation_50,50)
test_50 = set_50$test
test_50 = dataset_NAgenerator(test_50,50)

set_70 = split_dataset(data)
train_70 = set_70$train
train_70 = dataset_NAgenerator(train_70,70)
validation_70 = set_70$validation
validation_70 = dataset_NAgenerator(validation_70,70)
test_70 = set_70$test
test_70 = dataset_NAgenerator(test_70,70)


```
## Se entrenan y prueban con los 3 datasets
```{r}
#Esto es todo lo que deberiamos hacer oara cad auno de los data sets
test_dataset = function(train,valid,test,grid,porcentaje){
  modelo_optimizado = optimizacion(train, grid)

  best_auc = modelo_optimizado$AUC
  best_model = modelo_optimizado$model
  best_params = modelo_optimizado$hyperParameters
  results = modelo_optimizado$allResults
  
  #Validacion
  rendimiento_validacion = rendimiento_modelo(valid,best_model,paste("- Validación (NAs ", porcentaje,"%)"))
  matriz_confusion_validacion = rendimiento_validacion$matConfusion
  #print(matriz_confusion_validacion) paste("- Validación (NAs ", porcentaje,"%)")
  
  #Testeo
  rendimiento_testeo = rendimiento_modelo(test,best_model, paste("- Test (NAs ", porcentaje,"%)"))
  matriz_confusion_testeo = rendimiento_testeo$matConfusion
  #print(matriz_confusion_testeo)
  
  return (list(matConfValid = matriz_confusion_validacion,matConfTest = matriz_confusion_testeo))
}


matrices_data20 = test_dataset(train_20,validation_20,test_20,grid," 20")

print(matrices_data20$matConfValid)
print(matrices_data20$matConfTest)

matrices_data50 = test_dataset(train_50,validation_20,test_50,grid," 50")
matrices_data70 = test_dataset(train_70,validation_20,test_70,grid," 70")

```

##  Comparacion de los resultados

```{r}
# Creación del dataframe con las métricas de cada matriz de confusión
df_metricas <- data.frame(
  Porcentaje_Faltantes = c("0% (original)", "20%", "50%", "70%"),
  Accuracy = c(0.9634, 0.942, 0.9179, 0.8655),
  Sensitivity = c(0.9557, 0.9359, 0.8953, 0.7954),
  Specificity = c(0.9702, 0.9473, 0.9365, 0.9286),
  Kappa = c(0.9265, 0.8835, 0.8339, 0.7285)
)

print(df_metricas)
```

```{r}

# Convertir el dataframe a formato largo para ggplot2
df_metricas_largo <- melt(df_metricas, id.vars = "Porcentaje_Faltantes")

# Gráfico de barras
ggplot(df_metricas_largo, aes(x = Porcentaje_Faltantes, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Comparación de Métricas de Evaluación según el Porcentaje de Datos Faltantes",
       x = "Porcentaje de Datos Faltantes",
       y = "Valor de la Métrica",
       fill = "Métrica") +
  theme_minimal()
```

```{r}
# Gráfico de líneas
ggplot(df_metricas_largo, aes(x = Porcentaje_Faltantes, y = value, color = variable, group = variable)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(title = "Tendencia de las Métricas de Evaluación según el Porcentaje de Datos Faltantes",
       x = "Porcentaje de Datos Faltantes",
       y = "Valor de la Métrica",
       color = "Métrica") +
  theme_minimal()
```

## Analizar esto


# 8. Conclusiones y discusión

La gente debería consumir menos azúcar. También nos dimos cuenta de que, al ser un estudio realizado en Estados Unidos, se preguntó por la raza de los participantes, cuando en realidad esta variable influye poco en la predicción. Esto podría indicar que existe cierto sesgo hacia la raza en Estados Unidos.

El árbol de decisión es muy efectivo para el problema que planteamos, ya que el "accuracy" en la mayoría de las matrices de confusión fue alto, lo que indica que el modelo se ajusta bien a los datos y es capaz de predecir efectivamente si una persona tiene diabetes o no.

Algunas mejoras que podríamos considerar en nuestro trabajo incluyen explorar nuevos valores para los hiperparámetros, con el fin de descubrir si existe algún otro valor que pueda ajustar mejor el modelo. Sería interesante como posible dirección futura investigar la relación entre las personas que fuman y aquellas que tienen diabetes, ya que esta era la variable con más datos faltantes y podría proporcionarnos nuevas perspectivas sobre la diabetes.

A lo largo del proceso de construcción del modelo, las métricas clave siempre fueron altas durante las evaluaciones, lo que indica que el modelo se mantuvo consistentemente preciso y confiable.

Con base en los datos proporcionados y el análisis realizado del modelo de árbol de decisión, se pueden extraer las siguientes conclusiones generales:

1. *Efectividad del Modelo*: El modelo de árbol de decisión demuestra ser altamente efectivo para el problema planteado, que es la predicción de si una persona tiene diabetes o no. Esto se evidencia en los altos valores de precisión (accuracy) obtenidos en las matrices de confusión para diferentes subconjuntos de datos. Los resultados indican que el modelo se ajusta bien a los datos de entrenamiento y es capaz de generalizar adecuadamente a los datos de prueba.

2. *Impacto de los Valores Faltantes*: A medida que aumenta el porcentaje de valores faltantes en las variables predictoras, el rendimiento del modelo disminuye. Esto se refleja en la reducción de métricas clave como la precisión, la sensibilidad (recall) y el índice Kappa. Esta observación sugiere que el modelo es sensible a la calidad y la completitud de los datos; por lo tanto, la gestión adecuada de los valores faltantes es crucial para mantener el rendimiento del modelo.

3. *Sesgo Potencial en las Variables*: Durante el análisis se identificó que, aunque se recopilaron datos sobre la raza de los participantes, esta variable no tiene un impacto significativo en la predicción de diabetes. Esto podría indicar un posible sesgo en la recolección de datos, reflejando prácticas comunes en estudios realizados en Estados Unidos. Es importante considerar si el uso de esta variable es justificado o si podría introducir un sesgo innecesario en el modelo.

4. *Direcciones Futuras para Mejoras del Modelo*: El análisis sugiere que explorar diferentes configuraciones de hiperparámetros podría mejorar aún más el rendimiento del modelo. Por ejemplo, realizar una búsqueda más exhaustiva de hiperparámetros podría descubrir configuraciones que ajusten mejor el modelo a los datos. Además, la investigación sobre la relación entre fumar y la prevalencia de diabetes, especialmente considerando que esta fue una de las variables con más datos faltantes, podría proporcionar nuevas ideas para mejorar la precisión del modelo y la comprensión del problema.

5. *Consistencia del Modelo*: A lo largo del proceso de evaluación, el modelo de árbol de decisión ha mostrado un desempeño consistente. Las métricas clave, como la precisión y el índice Kappa, se mantuvieron altas, indicando que el modelo no solo es preciso, sino también estable. Esta consistencia sugiere que el árbol de decisión es una opción robusta para este tipo de análisis predictivo, capaz de manejar variaciones en los datos de manera eficaz.

6. *Posible Relación entre Fumar y Diabetes*: Dado que fumar podría estar relacionado con la diabetes y esta variable contenía datos faltantes significativos, investigarla más a fondo podría ofrecer nuevas perspectivas. Este análisis podría incluir la recolección de datos más completos o el uso de técnicas avanzadas de imputación para llenar los vacíos, lo que potencialmente podría mejorar el rendimiento del modelo.

En resumen, el modelo de árbol de decisión utilizado es adecuado para el problema de predicción de diabetes, pero hay áreas donde se podría mejorar, particularmente en la gestión de valores faltantes y la exploración de relaciones más profundas entre las variables predictoras. Estas mejoras podrían llevar a un modelo aún más preciso y útil en aplicaciones prácticas.